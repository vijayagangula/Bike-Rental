#!/usr/bin/env python
# coding: utf-8

# ## Dataset Information

# Bike sharing systems are a new version of traditional bike rentals where the whole
# process from membership, rental and return back has been automated. Through these
# systems, users are easily able to rent a bike from a particular position and return back
# at another position. Currently, there are about over 500 bike-sharing programs around
# the world which is composed of over 500 thousands bicycles. Today, there exists great
# interest in these systems due to their important role in traffic, environmental and health
# issues. 
# 
# Apart from interesting real world applications of bike sharing systems, the
# characteristics of data being generated by these systems make them attractive for
# research. Opposed to other transport services such as bus or subway, the duration of
# travel, departure and arrival position is explicitly recorded in these systems. This feature
# turns the bike sharing system into a virtual sensor network that can be used for sensing
# mobility in the city. Hence, it is expected that most of the important events in the city
# could be detected via monitoring these data.

# ## Objective

# - To select the suitable Regression algorithm for the bike sharing count and using it to predict the bike sharing count.
# - Identification of most promising attributes for the regression.

# ### Attribute Information

# Both the files hour.csv and day.csv have the following fields, except hr which is not
# available in day.csv
# - instant: record index
# - dteday : date
# - season : season (1:winter, 2:spring, 3:summer, 4:fall)
# - yr : year (0: 2011, 1:2012)
# - mnth : month ( 1 to 12)
# - hr : hour (0 to 23)
# - holiday : weather day is holiday or not (extracted from [Web Link])
# - weekday : day of the week
# - workingday : if neither weekend nor holiday, it is 1; otherwise 0.
# + weathersit :
# - 1: Clear, Few clouds, Partly cloudy, Partly cloudy
# - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
# - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered
# clouds
# - 4: Heavy Rain + Ice Pellets + Thunderstorm + Mist, Snow + Fog
# - temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-
# t_min), t_min=-8, t_max=+39 (only in hourly scale)
# - atemp: Normalized feeling temperature in Celsius. The values are derived via (t-
# t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)
# - hum: Normalized humidity. The values are divided to 100 (max)
# - windspeed: Normalized wind speed. The values are divided to 67 (max)
# - casual: count of casual users
# - registered: count of registered users
# - cnt: count of total rental bikes including both casual and registered.

# ### Import Modules

# In[1]:


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')


# ### Loading the dataset

# In[2]:


df = pd.read_csv('Hour.csv')
df.head()


# In[3]:


df.tail()


# In[4]:


df.cnt.value_counts()


# In[5]:


## Statistical information
df.describe()


# ## Descriptive Analysis

# Here initially we load the csv file and divide the attributes based on their feature such as categorical attributes and integer attributes.
# 
# This spitting will further be helpful for testing and training.
# 
# Later we find all the statistical information for each integer column.
# 
# Later for each column of the categorical attributes we find the variables like the count, unique values, top and the frequency.

# In[6]:


## Datatype Information
df.info()


# ### Preprocessing the dataset

# Data preprocessing refers to preparing (cleaning and organizing) the raw data to make it suitable for building and 
# training Machine Learning models.

# In[7]:


df.isnull().sum()


# There are no null values in the dataset.

# In[8]:


## Dropping unnecessary columns
df = df.drop(columns=['instant','dteday'],axis=1)


# In[9]:


df.duplicated().value_counts()


# In[10]:


df.drop_duplicates(inplace=True)


# In[11]:


corr=df.corr()
corr["hr"].sort_values(ascending=False)


# ### Exploratory Data Analysis (EDA)

# ##### We will analyze the data using visual techniques in terms of time and other attributes.

# In[12]:


fig, ax = plt.subplots(figsize=(20,10))
sns.pointplot(data =df,x='hr',y='cnt',hue='weekday',ax=ax)
ax.set(title='Count of bikes during weekdays and weekends')
plt.show()


# #### Inferences

# - The X-axis is the hour and Y-axis is the count of the bike.
# - On weekdays, we observe a peak in the morning hours and in the evening.
# - On weekends, the peak value is in the afternoon.

# In[13]:


fig, ax = plt.subplots(figsize=(20,10))
sns.pointplot(data =df,x='hr',y='casual',hue='weekday',ax=ax)
ax.set(title='Count of bikes during weekdays and weekends : Unregistered users')
plt.show()


# #### Inferences

# - The graph shows the count of unregistered users throughout the week.
# - We observe the high count on weekends.
# - This data can be related to weekend outdoor activities.

# In[14]:


fig, ax = plt.subplots(figsize=(20,10))
sns.pointplot(data =df,x='hr',y='registered',hue='weekday',ax=ax)
ax.set(title='Count of bikes during weekdays and weekends : Registered users')
plt.show()


# #### Inferences

# - The graph shows the count of registered users throughout the week.
# - This data can be related to the working personnel.
# - For the weekends they not using that much compared to the above graph ( Unregistered users)

# In[15]:


fig, ax = plt.subplots(figsize=(20,10))
sns.pointplot(data =df,x='hr',y='cnt',hue='weathersit',ax=ax)
ax.set(title='Count of bikes during different weathers')
plt.show()


# #### Inferences

# - The graph is similar to the previous graphs except for the weather 4.
# - Weather 4 with the color red must indicate Heavy Rain + Ice Pellets + Thunderstorm + Mist, Snow + Fog,
#   where no users book the bike.

# In[16]:


fig, ax = plt.subplots(figsize=(20,10))
sns.pointplot(data =df,x='hr',y='cnt',hue='season',ax=ax)
ax.set(title='Count of bikes during different seasons')
plt.show()


# #### Inferences

# - Out of four seasons three seasons those are spring, summer, fall have high number of users and have similar graph.
# - Only one season that is in winter have low number of users.

# In[17]:


fig, ax = plt.subplots(figsize=(10,5))
sns.barplot(data =df,x='mnth',y='cnt',ax=ax)
ax.set(title='Count of bikes during different months')
plt.show()


# #### Inferences

# - We observe that a uniform distribution.
# - Over a period of time the number of users are increases and gradually, the number of users decreases.

# In[18]:


fig, ax = plt.subplots(figsize=(10,5))
sns.barplot(data =df,x='weekday',y='cnt',ax=ax)
ax.set(title='Count of bikes during different days')
plt.show()


# #### Inferences

# - In this graph, we observe an average number of users throughout the week.
# - Thus, the average distribution is impractical for predictions.
# - We observe that all the days we are getting on an average of 175 to 180 number of vehicles.

# In[19]:


fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(20,6))
sns.regplot(x=df['temp'],y=df['cnt'],ax=ax1)
ax1.set(title = 'Relation between temperature and users')
sns.regplot(x=df['hum'],y=df['cnt'],ax=ax2)
ax2.set(title='Relation between humidity and users')
plt.show()


# - With the increase in temperature, the number of user increases.
# - When the humidity increases the number of users decreases.
# 

# ## Normality Test

# In[20]:


from statsmodels.graphics.gofplots import qqplot
fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(20,6))
sns.distplot(df['cnt'],ax=ax1)
ax1.set(title='Distribution of the users')
qqplot(df['cnt'],ax=ax2,line='s')
ax2.set(title='Theoritical quantiles')
plt.show()


# - We can see a huge numerical difference in the distribution of the users, so the data is not equally distributed, it is right skewed
# - Most of the data are in zero in the theoretical quantiles, so we must convert the data to approximate as much as possible as the red line

# ##### Now we will apply log transformation to uniform the data

# In[21]:


df['cnt'] = np.log(df['cnt'])


# In[22]:


fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(20,6))
sns.distplot(df['cnt'],ax=ax1)
ax1.set(title='Distribution of the users')
qqplot(df['cnt'],ax=ax2,line='s')
ax2.set(title='Theoritical quantiles')
plt.show()


# - Now the distribution is more uniform, meaning the data was converted accordingly.
# - Now the data in the theoretical quantiles is very similar to the red line.
# - we may use MIN-MAX normalization or Standardization to see different results.

# ### Correlation Matrix

# In[23]:


corr = df.corr()
plt.figure(figsize=(20,10))
sns.heatmap(corr,annot=True, annot_kws={'size':15})


# #### Inferences

# - We use the correlation matrix for numerical data.
# - We observe a highly positive correlation between 'temp' and 'atemp' and between 'casual' and 'registered'.
# - 'Windspeed' displays an insignificant contribution to the count.
# - Hence, we will drop a few unnecessary columns later.
# 

# ### Outlier Handling

# In[24]:


df_column = df.columns[~(df.columns=='cnt')]
for i in df_column:
    sns.boxplot(data=df,x=i)
    plt.title(f"Boxplot of {i}")
    plt.show()


# In[25]:


sns.boxplot(data=df,y='cnt',x='mnth',orient='v')
plt.show()


# In[26]:


sns.boxplot(data=df,y='cnt',x='season',orient='v')
plt.show()


# In[27]:


sns.boxplot(data=df,y='cnt',x='hr',orient='v')
plt.show()


# In[28]:


sns.boxplot(data=df,y='cnt',x='weathersit',orient='v')
plt.show()


# In[29]:


sns.boxplot(data=df,y='cnt',x='temp',orient='v')
plt.show()


# In[30]:


from scipy import stats


# In[31]:


IQR_win= stats.iqr(df.windspeed)
IQR_win


# In[32]:


Q1_win = np.percentile(df.windspeed,25)
Q3_win = np.percentile(df.windspeed,75)


# In[33]:


LB_win = Q1_win - 1.5*IQR_win
UB_win = Q3_win + 1.5*IQR_win


# In[34]:


df.loc[df.windspeed>UB_win,'windspeed']


# In[35]:


df.loc[df.windspeed<LB_win,'windspeed']


# In[36]:


df[(df.windspeed<UB_win)&(df.windspeed>LB_win)]


# In[37]:


((17379-17037)/17379)*100


# In[38]:


df.windspeed.median()


# In[39]:


df.loc[df.windspeed>UB_win,'windspeed'] = 0.194


# In[40]:


IQR_ca=stats.iqr(df.casual)
IQR_ca


# In[41]:


Q1_ca = np.percentile(df.casual,25)
Q3_ca =np.percentile(df.casual,75)


# In[42]:


LB_ca = Q1_ca - 1.5*IQR_ca
UB_ca = Q3_ca +1.5*IQR_ca


# In[43]:


df.loc[df.casual>UB_ca,'casual']


# In[44]:


df.loc[df.casual<LB_ca,'casual']


# In[45]:


df[(df.casual<UB_ca)&(df.casual>LB_ca)]


# In[46]:


((17379-16170)/17379)*100


# In[47]:


IQR_re = stats.iqr(df.registered)
IQR_re


# In[48]:


Q1_re = np.percentile(df.registered,25)
Q3_re = np.percentile(df.registered,75)


# In[49]:


LB_re = Q1_re - 1.5 *IQR_re
UB_re = Q3_re + 1.5*IQR_re


# In[50]:


df.loc[df.registered>UB_re,'registered']


# In[51]:


df.loc[df.registered<LB_re,'registered']


# In[52]:


df[(df.registered<UB_re)&(df.registered>LB_re)]


# In[53]:


((17379-16696)/17379)*100


# In[54]:


df.registered.median()


# In[55]:


df.loc[df.registered>UB_re,'registered']=115.0


# In[56]:


## Change int columns to category
cols = ['season','mnth','hr','holiday','weekday','workingday','weathersit']
for i in cols:
    df[i]=df[i].astype('category')


# In[57]:


df.info()


# ### Encoding

# In[58]:


df= pd.get_dummies(df, columns=cols)


# In[59]:


df.head()


# - New data frame after encoding the data, adding new features.
# - With the additional features added this will increase the training process time as well as the accuracy.
# 

# ## Scaling

# In[60]:


from sklearn.preprocessing import MinMaxScaler


# In[61]:


scale = MinMaxScaler()
df[['temp','atemp','hum','windspeed','casual','registered','cnt']]=scale.fit_transform(df[['temp','atemp','hum','windspeed','casual','registered','cnt']])


# In[62]:


df


# ## Split Data

# In[63]:


X = df[df.columns.difference(['cnt'])]
y = df['cnt']


# ## Model Training

# In[64]:


from sklearn.linear_model import LinearRegression, Ridge, HuberRegressor, ElasticNetCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,ExtraTreesRegressor
from sklearn.svm import SVR

models = [LinearRegression(),
         Ridge(),
         HuberRegressor(),
         ElasticNetCV(),
         DecisionTreeRegressor(),
         RandomForestRegressor(),
         GradientBoostingRegressor(),
         ExtraTreesRegressor(),
         SVR()]


# In[65]:


from sklearn import model_selection


# In[66]:


def train(model):
    kfold = model_selection.KFold(n_splits=5)
    pred = model_selection.cross_val_score(model,X,y,cv=kfold,scoring='neg_mean_squared_error')
    cv_score = pred.mean()
    print('Model:' , model)
    print('CV score:', abs(cv_score))


# In[67]:


for model in models:
    train(model)


# - Various models were imported to see different results
# - These are common models for regression problems.
# - Which model has the least CV score that model is better model.
# - As seen in above Random Forest Regressor is best model than the other models.
# - Now we can train the models who have less CV score.

# In[68]:


from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)


# ## Model Evaluation

# In[69]:


from sklearn.metrics import r2_score


# In[70]:


from sklearn.metrics import mean_squared_error,mean_absolute_error


# # Random Forest Regressor 

# In[71]:


r_model = RandomForestRegressor()
r_model.fit(X_train,y_train)
r_y_pred = r_model.predict(X_test)


# ### Error Difference

# In[72]:


error = y_test - r_y_pred
fig,ax = plt.subplots()
ax.scatter(y_test,error)
ax.axhline(lw=3,color = 'black')
ax.set_xlabel('Observed')
ax.set_ylabel('Error')
plt.show()


# In[73]:


mean_squared_error(y_test,r_y_pred)


# In[74]:


np.sqrt(mean_squared_error(y_test,r_y_pred))


# In[75]:


mean_absolute_error(y_test,r_y_pred)


# - Mean squared error from the test data and the predicted data

# In[76]:


r2_score(y_test,r_y_pred)


# In[77]:


r_model.feature_importances_


# In[78]:


imp =r_model.feature_importances_
indices = np.argsort(imp)
features =X_train.columns


# In[79]:


plt.figure(figsize=(10,15))
plt.title('Feature Importance')
plt.barh(range(len(indices)),imp[indices],align = 'center',color='Green')
plt.yticks(range(len(indices)),[features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()


# # Decision Tree Regressor

# In[80]:


d_model = DecisionTreeRegressor()
d_model.fit(X_train,y_train)
d_y_pred = d_model.predict(X_test)


# ### Error Difference

# In[81]:


error = y_test - d_y_pred
fig,ax = plt.subplots()
ax.scatter(y_test,error)
ax.axhline(lw=3,color = 'black')
ax.set_xlabel('Observed')
ax.set_ylabel('Error')
plt.show()


# In[82]:


mean_squared_error(y_test,d_y_pred)


# In[83]:


np.sqrt(mean_squared_error(y_test,d_y_pred))


# In[84]:


mean_absolute_error(y_test,d_y_pred)


# In[85]:


r2_score(y_test,d_y_pred)


# In[86]:


d_model.feature_importances_


# In[87]:


d_imp = d_model.feature_importances_
d_indices = np.argsort(d_imp)
features =X_train.columns


# In[88]:


plt.figure(figsize=(10,20))
plt.title('Feature Importance')
plt.barh(range(len(d_indices)),d_imp[d_indices],color='blue',align = 'center')
plt.yticks(range(len(d_indices)),[features[i] for i in d_indices])
plt.xlabel('Relative Importance')
plt.show()


# # Extra Trees Regressor

# In[89]:


e_model = ExtraTreesRegressor()
e_model.fit(X_train,y_train)
e_y_pred = e_model.predict(X_test)


# ### Error Difference

# In[90]:


error = y_test - e_y_pred
fig,ax = plt.subplots()
ax.scatter(y_test,error)
ax.axhline(lw=3,color = 'black')
ax.set_xlabel('Observed')
ax.set_ylabel('Error')
plt.show()


# In[91]:


mean_squared_error(y_test,e_y_pred)


# In[92]:


np.sqrt(mean_squared_error(y_test,e_y_pred))


# In[93]:


mean_absolute_error(y_test,e_y_pred)


# In[94]:


r2_score(y_test,e_y_pred)


# # Gradient Boosting Regressor

# In[95]:


g_model = GradientBoostingRegressor()
g_model.fit(X_train,y_train)
g_y_pred = g_model.predict(X_test)


# ### Error Difference

# In[96]:


error = y_test - g_y_pred
fig,ax = plt.subplots()
ax.scatter(y_test,error)
ax.axhline(lw=3,color = 'black')
ax.set_xlabel('Observed')
ax.set_ylabel('Error')
plt.show()


# In[97]:


mean_squared_error(y_test,g_y_pred)


# In[98]:


np.sqrt(mean_squared_error(y_test,g_y_pred))


# In[99]:


mean_absolute_error(y_test,g_y_pred)


# In[100]:


r2_score(y_test,g_y_pred)


# # SVR

# In[101]:


s_model = SVR()
s_model.fit(X_train,y_train)
s_y_pred = s_model.predict(X_test)


# ### Error Difference

# In[102]:


error = y_test - s_y_pred
fig,ax = plt.subplots()
ax.scatter(y_test,error)
ax.axhline(lw=3,color = 'black')
ax.set_xlabel('Observed')
ax.set_ylabel('Error')
plt.show()


# In[103]:


mean_squared_error(y_test,s_y_pred)


# In[104]:


np.sqrt(mean_squared_error(y_test,s_y_pred))


# In[105]:


mean_absolute_error(y_test,s_y_pred)


# In[106]:


r2_score(y_test,s_y_pred)


# # Elastic Net CV

# In[107]:


ela_model = ElasticNetCV()
ela_model.fit(X_train,y_train)
ela_y_pred = ela_model.predict(X_test)


# ### Error Difference

# In[108]:


error = y_test - ela_y_pred
fig,ax = plt.subplots()
ax.scatter(y_test,error)
ax.axhline(lw=3,color = 'black')
ax.set_xlabel('Observed')
ax.set_ylabel('Error')
plt.show()


# In[109]:


mean_squared_error(y_test,ela_y_pred)


# In[110]:


np.sqrt(mean_squared_error(y_test,ela_y_pred))


# In[111]:


mean_absolute_error(y_test,ela_y_pred)


# In[112]:


r2_score(y_test,ela_y_pred)


# # Linear Regression

# In[113]:


l_model = LinearRegression()
l_model.fit(X_train,y_train)
l_y_pred = l_model.predict(X_test)


# ### Error Difference

# In[114]:


error = y_test - l_y_pred
fig,ax = plt.subplots()
ax.scatter(y_test,error)
ax.axhline(lw=3,color = 'black')
ax.set_xlabel('Observed')
ax.set_ylabel('Error')
plt.show()


# In[115]:


mean_squared_error(y_test,l_y_pred)


# In[116]:


np.sqrt(mean_squared_error(y_test,l_y_pred))


# In[117]:


mean_absolute_error(y_test,l_y_pred)


# In[118]:


r2_score(y_test,l_y_pred)


# # Ridge

# In[119]:


rid_model = Ridge()
rid_model.fit(X_train,y_train)
rid_y_pred = rid_model.predict(X_test)


# ### Error Difference

# In[120]:


error = y_test - rid_y_pred
fig,ax = plt.subplots()
ax.scatter(y_test,error)
ax.axhline(lw=3,color = 'black')
ax.set_xlabel('Observed')
ax.set_ylabel('Error')
plt.show()


# In[121]:


mean_squared_error(y_test,rid_y_pred)


# In[122]:


np.sqrt(mean_squared_error(y_test,rid_y_pred))


# In[123]:


mean_absolute_error(y_test,rid_y_pred)


# In[124]:


r2_score(y_test,rid_y_pred)


# # Huber Regressor

# In[125]:


h_model = HuberRegressor()
h_model.fit(X_train,y_train)
h_y_pred = h_model.predict(X_test)


# ### Error Difference

# In[126]:


error = y_test - h_y_pred
fig,ax = plt.subplots()
ax.scatter(y_test,error)
ax.axhline(lw=3,color = 'black')
ax.set_xlabel('Observed')
ax.set_ylabel('Error')
plt.show()


# In[127]:


mean_squared_error(y_test,h_y_pred)


# In[128]:


np.sqrt(mean_squared_error(y_test,h_y_pred))


# In[129]:


mean_absolute_error(y_test,h_y_pred)


# In[130]:


r2_score(y_test,h_y_pred)


# ## Models Evaluation Table

# |S.No| Algorithms | Absolute CV Score | MSE | RMSE | MAE | r2_score|
# |----|------------|-------------------|-----|------|-----|---------|
# |1|Random Forest Regressor |6.61e-05|3.12e-05    |  0.0055     |  0.0011  |  0.9993    |
# |2|Decision Tree Regressor|0.0001|  6.60e-05  |  0.0081     |  0.0020   | 0.9986     |
# |3|Extra Tree Regressor|  7.66e-05        |  6.04e-05     | 0.0077      | 0.0015    | 0.9987     |
# |4|Gradient Boosting Regressor|0.0001       | 0.0001    | 0.0113     | 0.0058    | 0.9972      |
# |5|SVR|   0.0040                           | 0.0032     |0.0570      | 0.0447    | 0.9309      |
# |6|Elastic Net CV| 0.0069                 | 0.0063      |  0.0795    |  0.0572   |  0.8660     |
# |7|Linear Regression| 0.0069        |0.0063         | 0.0795     |  0.0574    | 0.8658     |
# |8|Ridge|         0.0069            |0.0063     |0.0795        |0.0572      |0.8659|
# |9|Huber Regressor|   0.0077       |0.0067     |  0.0821        |   0.0536        |0.8571       |
# 

# # Data Analysis Report

# I completed this job using the steps listed below.

# #### 1. Information of the dataset.
# #### 2. Objective of the dataset.
# #### 3. Attribute Information.
# #### 4. Import Modules.
# #### 5. Loading the dataset.
# #### 6. Descriptive Analysis
# #### 7. Preprpcessing the dataset
#     (a) Check the null values
#     (b) Drop unnecessary columns
#     (c) Check correlation
# #### 8. Exploratory Data Analysis
#    In this we use:
#   
#         (a) pointplot
#         (b) barplot
#         (c) regplot
#          And given inferences about each plot for the dataset.
#     
# #### 9. Normality Test
#    In this we use:
#    
#         (a) qqplot
#         (b) distplot
# #### 10. Crrelation Matrix
#    In this we use Heatmap and given inferences for the dataset.
# #### 11. Outlier Handling
#       By using IQR method we detect outliers and solve them. 
# #### 12. Encoding
#     In this we use One-Hot Encoding.
# #### 13. Scaling
#     In this we use MinmaxScalar.
# #### 14. Split Data
#     We split 80% of data for training and 20% of data for testing.
# #### 15. Model Training
#     Models Used : LinearRegression, Ridge, HuberRegressor, ElasticNetCV, DecisionTreeRegressor, RandomForestRegressor,
#                   GradientBoostingRegressor, ExtraTreesRegressor, SVR.
#             
#     We see absolute CV scores of the each model. Which model have the least absolute CV score that is the best model 
#     and that is Random Forest Regressor.
#     We also seen Feature Importance.
# #### 16. Model Evaluation
#     In this we evaluate MSE, RMSE, MAE and r2_score of each model.
# #### 17. Model Evaluation Table.
#      In this MSE     : Mean Square Error
#              RMSE    : Root Mean Square Error
#              MAE     : Mean Absolute Error
#              CV Score: Cross Validation Score
#              

# ## Model Comparision Report

# #####  Based on model evaluation Random Forest Regressor have the best and highest r2_score.
# #####  Extra Trees Regressor algorithm can be considered as second most good model.

# ### Challenges Faced

# Certain challenges faced to uderstand the data factors. And then I undestood.

# # Conclusion and Future Work

# Finally we can predict that the attributes registered and casual variables are the factors that influence the most of the
# bike sharing count data set using which we know their prominence. We can also state that the use of Random Forest for this
# kind of data set is the best way to accurately predict the influence of which attributes are more and which are less. Similarly
# if you consider the method where we use the data set, we can conclude that the training data set is always the best data set.
# Where as we find MSE, RMSE, MAE and r2 score whereas in all the means we consider mean of the MAE as the factor to analyze the 
# Random Forest Regression Model.
# 
# For the large data sets, the implementation of the Random Forest Regression using sklearn will slow down the process which
# is due to the more computation cost and also because the storage of the data in the main memory can not be completely stored.
# In certain scenarios this sklearn leads to the worst implementation that crashes and thus not advised for large data sets.
# 
# There are many other alternatives for this problem, but the best one will be the python woodly implementation in which we use
# the top tree pre classification and that also distributes the samples to the bottom of the Random Forsts That are implemented
# using C and this is also a highly optimised solution. We can use other machine learning frameworks such as Apache Spark ML for
# highly optimised distributed computation which will help in the utilization of computer clustring. This Spark ML can run Kubernetes,
# Apache Mesos, Hadloop etc, which will access the data from various popular Apache databases such as Apache Cassandra which is 
# 100 times faster than any classic algorithms.
# 
# To improve the performance of the model, we adjust the distribution of the target variable that is, if we see some of the predictive
# models, they assume the normal distribution of the target variable which can be improved by simply implementing a transformation
# in the data pre-processing which further improves the performance of the such methods.

# In[ ]:




